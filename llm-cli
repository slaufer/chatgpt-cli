#!/usr/bin/python3
import os
import sys
import json
import base64
import argparse
import mimetypes
import anthropic

from shutil import get_terminal_size
from openai import OpenAI

from prompt_toolkit import prompt
from prompt_toolkit.key_binding import KeyBindings

DEFAULT_SYSTEM_PROMPT = f'''
Carefully heed the user's instructions.
Respond using Markdown.
Respond briefly and concisely unless you are instructed to do otherwise.
Do not include any extraneous or tangential details unless you are instructed to do otherwise.
'''.strip()

INTERACTIVE_KEYS = 'Press Alt+Enter to submit; Ctrl+F to add file; Ctrl+I to add image; Ctrl+S to get completion without message; Ctrl+C or Ctrl+D to exit.'

#####################
# UTILITY FUNCTIONS #
#####################

def get_file(file_path):
  path = os.path.relpath(os.path.abspath(file_path), os.getcwd())

  with open(path, 'r') as file:
    content = file.read()

  return path, content

def get_mime_type(image_path):
  mime_type, _ = mimetypes.guess_type(image_path)

  if mime_type is None:
    return 'image/jpeg'
  
  return mime_type

##################
# MODEL ADAPTERS #
##################

class BaseModelAdapter:
  def __init__():
    pass

  def get_completion(messages):
    pass

OPENAI_MODELS = [
  "gpt-4-0125-preview",
  "gpt-4-turbo-preview",
  "gpt-4-1106-preview",
  "gpt-4o-2024-05-13",
  "gpt-4",
  "gpt-3.5-turbo",
  "gpt-3.5-turbo-1106",
  "gpt-3.5-turbo-16k",
  "gpt-4o-mini",
  "gpt-4o-mini-2024-07-18",
  "chatgpt-4o-latest",
  "gpt-3.5-turbo-instruct-0914",
  "gpt-3.5-turbo-0125",
  "gpt-4o-2024-08-06",
  "gpt-4-0613",
  "gpt-3.5-turbo-instruct",
  "gpt-4o",
  "gpt-4-turbo-2024-04-09",
  "gpt-4-turbo"
]

class OpenAiModelAdapter(BaseModelAdapter):
  def __init__(
    self,
    model,
    api_key,
    max_tokens=None,
    temperature=None,
    top_p=None,
    frequency_penalty=None,
    presence_penalty=None
  ):
    self.model = model
    self.client = OpenAI(api_key=api_key)
    self.max_tokens = max_tokens
    self.temperature = temperature
    self.top_p = top_p
    self.frequency_penalty = frequency_penalty
    self.presence_penalty = presence_penalty

  def get_completion(self, input_messages):
    messages = []

    for message in input_messages:
      if message.get('role') == 'file':
        messages.append({
          "role": "user",
          "content": message.get('file_content')
        })
      elif message.get('role') == 'image':
        if self.max_tokens is None:
          self.max_tokens = 300

        messages.append({
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": 'IMAGE: ' + message.get('content')
            },
            {
              "type": "image_url",
              "image_url": {
                "url": 'data:' + get_mime_type(message.get('content')) + ';base64,' + message.get('image_content')
              }
            }
          ]
        })
      else:
        messages.append(message)

    req = {
      "messages": messages,
      "model": self.model,
    }

    if self.max_tokens is not None:
      req["max_tokens"] = self.max_tokens
    
    if self.temperature is not None:
      req["temperature"] = self.temperature

    if self.top_p is not None:
      req["top_p"] = self.top_p

    if self.frequency_penalty is not None:
      req["frequency_penalty"] = self.frequency_penalty

    if self.presence_penalty is not None:
      req["presence_penalty"] = self.presence_penalty

    response = self.client.chat.completions.create(**req)
    return response.choices[0].message.content

ANTHROPIC_MODELS = [
  "claude-3-5-sonnet-20240620",
  "claude-3-opus-20240229",
  "claude-3-sonnet-20240229",
  "claude-3-haiku-20240307",
  "claude-2.1",
  "claude-2.0",
  "claude-instant-1.2"
]

class AnthropicModelAdapter(BaseModelAdapter):
  def __init__(
    self,
    model,
    api_key,
    max_tokens=None,
    temperature=None,
    top_p=None
  ):
    self.system = ""
    self.model = model
    self.client = anthropic.Anthropic(api_key=api_key)
    self.max_tokens = max_tokens
    self.temperature = temperature
    self.top_p = top_p

  def get_completion(self, input_messages):
    messages = []

    for message in input_messages:
      if message.get('role') == 'system':
        self.system = message.get('content')
        continue

      if message.get('role') == 'file':
        out_message = {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": message.get('file_content')
            }
          ]
        }
      elif message.get('role') == 'image':
        out_message = {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": 'IMAGE: ' + message.get('content')
            },
            {
              "type": "image",
              "source": {
                "type": "base64",
                "media_type": get_mime_type(message.get('content')),
                "data": message.get('image_content')
              }
            }
          ]
        }
      else:
        out_message = {
          "role": message.get('role'),
          "content": [
            {
              "type": "text",
              "text": message.get('content')
            }
          ]
        }

      # Merge consecutive messages of the same role (required by Anthropic API)
      if len(messages) > 0 and messages[-1].get('role') == out_message.get('role'):
        messages[-1]['content'] += out_message.get('content')
      else:
        messages.append(out_message)

    req = {}

    if self.temperature is not None:
      req['temperature'] = self.temperature

    if self.max_tokens is not None:
      req['max_tokens'] = self.max_tokens
    else:
      req['max_tokens'] = 300

    if self.top_p is not None:
      req['top_p'] = self.top_p

    if self.system is not None:
      req['system'] = self.system

    req['model'] = self.model
    req['messages'] = messages

    response = self.client.messages.create(**req)
    return response.content[0].text


def get_model_adapter(name, params):
  api = params.get('api')

  if api == 'openai' or (api is None and name in OPENAI_MODELS):
    return OpenAiModelAdapter(
      model = name,
      api_key = os.environ.get('OPENAI_API_KEY'),
      max_tokens = params.get('max_tokens'),
      temperature = params.get('temperature'),
      top_p = params.get('top_p'),
      frequency_penalty = params.get('frequency_penalty'),
      presence_penalty = params.get('presence_penalty')
    )
  
  if api == 'anthropic' or (api is None and name in ANTHROPIC_MODELS):
    return AnthropicModelAdapter(
      model = name,
      api_key = os.environ.get('ANTHROPIC_API_KEY'),
      max_tokens = params.get('max_tokens'),
      temperature = params.get('temperature'),
      top_p = params.get('top_p')
    )

  raise Exception('No valid model was selected.')

##############
# CLI DRIVER #
##############

class LlmCli:
  def __init__(
    self,
    log_file=None,
    log_file_json=None,
    interactive=True,
    immediate=False,
    separator = None,
    intro=True,
    no_system_prompt=False,
    model_adapter=None
  ):
    self.log_file = log_file
    self.json_log_file = log_file_json
    self.interactive = interactive
    self.immediate = immediate
    self.separator = separator
    self.intro = intro
    self.no_system_prompt = no_system_prompt
    self.model_adapter = model_adapter
    self.messages = []

  def log_json(self):
      """Dump the messages to a JSON file."""
      if self.json_log_file:
        with open(self.json_log_file, 'w') as file:
            json.dump(self.messages, file, indent=4)

  def output(self, message, silent=False):
    if not silent:
      print(message, end='', flush=True)

    if self.log_file is not None:
      with open(self.log_file, 'a') as file:
        file.write(message)

  def get_completion(self):
    return self.model_adapter.get_completion(self.messages)

  def get_separator(self):
    if self.separator is not None:
      return self.separator
    
    return '\n\n #' + ('=' * (get_terminal_size().columns - 4)) + '#\n\n'

  def add_chat_message(self, role, content, message=None, silent=False):
    if role == 'file':
      if message and 'file_content' in message:
        content = message.get('content')
        file_content = message.get('file_content')
      else:
        file_path = os.path.relpath(os.path.abspath(content), os.getcwd())
        content = file_path
        with open(file_path, 'r') as file:
          file_content = '### FILE: `{file_path}`\n```\n' + file.read() + '\n```'

      message = {
        "role": "file",
        "content": content,
        "file_content": file_content
      }
      self.messages.append(message)
      self.output('FILE: ' + message.get('content') + ' (contents hidden)', silent=silent)
    elif role == 'image':
      if message and 'image_content' in message:
        content = message.get('content')
        image_content = message.get('image_content')
      else:
        file_path = os.path.relpath(os.path.abspath(content), os.getcwd())
        content = file_path
        with open(file_path, 'rb') as file:
          image_content = base64.b64encode(file.read()).decode('utf-8')
      message = {
        "role": "image",
        "content": content,
        "image_content": image_content
      }
      self.messages.append(message)
      self.output('IMAGE: ' + message.get('content') + ' (contents hidden)', silent=silent)
    else:
      self.messages.append({"role": role, "content": content})
      self.output(f'{role.capitalize()}:\n\n{content}', silent=silent)
    
    self.output(self.get_separator(), silent=silent)

  def add_messages_from_args(self, args):
    silent = not self.interactive or not self.intro

    args_iter = iter(args)
    args_messages = []

    for arg in args_iter:
      if arg not in ('-s', '--system', '-a', '--assistant', '-u', '--user', '-f', '--file', '-i', '--image', '-c', '--conversation'):
        continue

      content = next(args_iter, None)

      if arg in ('-s', '--system'):
        args_messages.append({"role": "system", "content": content})
      elif arg in ('-a', '--assistant'):
        args_messages.append({"role": "assistant", "content": content})
      elif arg in ('-u', '--user'):
        args_messages.append({"role": "user", "content": content})
      elif arg in ('-f', '--file'):
        args_messages.append({"role": "file", "content": content})
      elif arg in ('-i', '--image'):
        args_messages.append({"role": "image", "content": content})
      elif arg in ('-c', '--conversation'):
        # so you can combine -c and -j on a file that doesn't exist yet
        if not os.path.exists(content) and self.json_log_file == content:
          continue

        with open(content, 'r') as file:
          conversation = json.load(file)
          for message in conversation:
            args_messages.append(message)

    if not self.no_system_prompt and not any(message.get("role") == "system" for message in args_messages):
      args_messages.insert(0, {"role": "system", "content": DEFAULT_SYSTEM_PROMPT})

    for message in args_messages:
      self.add_chat_message(message.get('role'), message.get('content'), message=message, silent=silent)

  def main(self, args):
    if self.interactive:
        self.output(f'{INTERACTIVE_KEYS}' + self.get_separator())

    self.add_messages_from_args(args)

    if not self.interactive:
      response = self.get_completion()
      self.add_chat_message("assistant", response, silent=True)
      self.log_json()
      self.output(response)
      return
    
    if self.immediate:
      response = self.get_completion()
      self.add_chat_message("assistant", response)

    self.log_json()

    bindings = KeyBindings()
    bindings.add('c-c')(lambda _: sys.exit(0))
    bindings.add('c-d')(lambda _: sys.exit(0))
    bindings.add('enter')(lambda event: event.app.current_buffer.insert_text('\n'))
    bindings.add('escape', 'enter')(lambda event: event.app.exit(result=(event.app.current_buffer.text, 'text')))
    bindings.add('c-s')(lambda event: event.app.exit(result=(None, 'skip')))
    bindings.add('c-i')(lambda event: event.app.exit(result=(event.app.current_buffer.text, 'image')))
    bindings.add('c-f')(lambda event: event.app.exit(result=(event.app.current_buffer.text, 'file')))

    while True:
      (user_input, user_input_type) = prompt(
        'User:\n\n',
        multiline=True,
        key_bindings=bindings
      )

      try:
        if user_input_type == 'skip':
          pass
        elif user_input_type == 'text':
          self.add_chat_message('user', user_input, silent=True)
        elif user_input_type == 'file':
          self.add_chat_message('file', user_input)
        elif user_input_type == 'image':
          self.add_chat_message('image', user_input)
      except Exception as e:
        self.output(f'Unable to add message: {str(e)}\n\n')
        continue

      self.log_json()


      if user_input_type in ['text', 'skip']:
        self.output(self.get_separator())

        try:
          response = self.get_completion()
          self.add_chat_message("assistant", response)
        except Exception as e:
          self.output(f'Unable to get completion: {str(e)}\n\n')
          continue

        self.log_json()

################
# HELP MESSAGE #
################

def print_help():
  exec_path = os.path.basename(sys.argv[0])
  print(f"""
LLM CLI

This command line interface allows you to interact with OpenAI and Anthropic's chat completion APIs in various ways, including sending system prompts, assistant responses, user prompts, and loading previous conversations. It supports both interactive and non-interactive modes.

Usage:
  {exec_path} [options]

Options:
  MESSAGE ARGUMENTS:
  -s, --system <message>       Add a system prompt message. If not specified, a default system prompt is used.
  -a, --assistant <message>    Add an assistant response message.
  -u, --user <message>         Add a user prompt message.
  -f, --file <filename>        Add a user prompt message from a file.
  -i, --image <filename>       Add a user prompt message from an image. If --max-tokens is not specified, a default value may be applied.
  -c, --conversation <file>    Load a previous conversation from a file. (see -j / --log-file-json)
  -d, --no-system-prompt       Don't add a default system prompt if none is present.
  
  MODEL ARGUMENTS:
  --list-models                List available models for automatic API selection.
  -m, --model <model_name>     Specify the model to use. (default: gpt-4o)
  --api                        Specify the API to use. Required for unrecognized models. (options: openai, anthropic)
  --temperature <value>        The sampling temperature to use, between 0 and 1.
  --max-tokens <number>        The maximum number of tokens to generate in the completion.
  --top-p <value>              The top-p sampling value to use, between 0 and 1.
  --frequency-penalty <value>  The frequency penalty to use, between -2.0 and 2.0. (OpenAI models only)
  --presence-penalty <value>   The presence penalty to use, between -2.0 and 2.0. (OpenAI models only)
        
  OTHER ARGUMENTS:
  -n, --non-interactive        Disable interactive mode, get a completion and exit. Use message arguments to specify the conversation.
  -l, --log-file <filename>    Log output to a specified file.
  -j, --log-file-json <file>   Output a JSON-formatted log to a specified file.
  -g, --immediate              Get an assistant response immediately, before entering interactive mode.
  -x, --separator <separator>  Specify the separator to use between messages.
  -o, --no-intro               Don't print the system prompt, or messages specified on the command line.
  -h, --help                   Print this help message and exit.

Message arguments are added to the conversation in the order in which they are specified on the command line. If no system prompt is specified, a default prompt will be added.

By default, the program begins in interactive mode. Interactive mode uses a multi-line editor. {INTERACTIVE_KEYS}

Make sure you set the appropriate API key environment variable. For OpenAI models, set OPENAI_API_KEY. For Anthropic models, set ANTHROPIC_API_KEY.

TIP: Try `{exec_path} -c mylog.json -j mylog.json` to persist conversations between sessions.
        """.strip())

def print_model_list():
  print("OpenAI models:")
  for model in OPENAI_MODELS:
    print(f"  - {model}")
    
  print("\nAnthropic models:")
  for model in ANTHROPIC_MODELS:
    print(f"  - {model}")
  return

###################
# ARGUMENT PARSER #
###################

def get_args():
  parser = argparse.ArgumentParser(
    description='ChatGPT CLI',
    add_help=False,
  )

  # Message arguments
  # Argparse is just validating these, we parse them manually below
  parser.add_argument('-s', '--system', action='append')
  parser.add_argument('-a', '--assistant', action='append')
  parser.add_argument('-u', '--user', action='append')
  parser.add_argument('-f', '--file', action='append')
  parser.add_argument('-i', '--image', action='append')
  parser.add_argument('-c', '--conversation', action='append')
  parser.add_argument('-d', '--no-system-prompt', action='store_true')

  # Model argumhttps://github.com/acv-auctions/acv-web-vuejsents
  parser.add_argument('--list-models', action='store_true')
  parser.add_argument('--api', choices=['openai', 'anthropic'])
  parser.add_argument('--temperature', type=float)
  parser.add_argument('--max-tokens', type=int)
  parser.add_argument('--top-p', type=float)
  parser.add_argument('--frequency-penalty', type=float)
  parser.add_argument('--presence-penalty', type=float)

  # Other arguments
  parser.add_argument('-n', '--non-interactive', action='store_true')
  parser.add_argument('-l', '--log-file')
  parser.add_argument('-j', '--log-file-json')
  parser.add_argument('-m', '--model', default='gpt-4o')
  parser.add_argument('-g', '--immediate', action='store_true')
  parser.add_argument('-x', '--separator')
  parser.add_argument('-o', '--no-intro', action='store_true')
  parser.add_argument('-h', '--help', action='store_true')

  return parser.parse_args()


###############
# INITIALIZER #
###############

def main():
  args = get_args()

  if args.help:
    print_help()
    return
  
  if args.list_models:
    print_model_list()
    return

  if (os.environ.get("OPENAI_API_KEY") is None):
    print("Please set the OPENAI_API_KEY environment variable to your OpenAI API key. Use -h for help.")
    sys.exit(1)

  model_adapter = get_model_adapter(args.model, {
    "temperature": args.temperature,
    "max_tokens": args.max_tokens,
    "top_p": args.top_p,
    "frequency_penalty": args.frequency_penalty,
    "presence_penalty": args.presence_penalty,
    "api": args.api,
  })

  cli = LlmCli(
    log_file=args.log_file,
    log_file_json=args.log_file_json,
    interactive=not args.non_interactive,
    immediate=args.immediate,
    separator=args.separator,
    intro=not args.no_intro,
    no_system_prompt=args.no_system_prompt,
    model_adapter=model_adapter
  )

  cli.main(sys.argv[1:])

if __name__ == "__main__":
  main()
